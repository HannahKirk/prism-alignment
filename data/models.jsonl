{"long_name": "claude-2", "short_name": "claude-2", "longcode": "CL2", "model_family": "Anthropic", "model_provider": "Anthropic", "model_provider_type": "Commerical", "model_type": "Chat", "default_params": {"temperature": "1.0", "top_p": "0.7", "presence_penalty": "0.0", "frequency_penalty": "0.0", "max_tokens": "256", "top_k": "5"}, "selected_params": {"temperature": "1.0", "top_p": "0.7", "presence_penalty": "0.0", "frequency_penalty": "0.0", "max_tokens": "256", "top_k": "5"}, "header": "You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://docs.anthropic.com/claude/docs/legacy-model-guide", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully"}
{"long_name": "claude-2.1", "short_name": "claude-2.1", "longcode": "CL2*", "model_family": "Anthropic", "model_provider": "Anthropic", "model_provider_type": "Commerical", "model_type": "Chat", "default_params": {"temperature": "1.0", "top_p": "0.7", "presence_penalty": "0.0", "frequency_penalty": "0.0", "max_tokens": "256", "top_k": "5"}, "selected_params": {"temperature": "1.0", "top_p": "0.7", "presence_penalty": "0.0", "frequency_penalty": "0.0", "max_tokens": "256", "top_k": "5"}, "header": "You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://docs.anthropic.com/claude/docs/legacy-model-guide", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully"}
{"long_name": "claude-instant-1", "short_name": "claude-instant-1", "longcode": "CL1", "model_family": "Anthropic", "model_provider": "Anthropic", "model_provider_type": "Commerical", "model_type": "Chat", "default_params": {"temperature": "1.0", "top_p": "0.7", "presence_penalty": "0.0", "frequency_penalty": "0.0", "max_tokens": "256", "top_k": "5"}, "selected_params": {"temperature": "1.0", "top_p": "0.7", "presence_penalty": "0.0", "frequency_penalty": "0.0", "max_tokens": "256", "top_k": "5"}, "header": "You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://docs.anthropic.com/claude/docs/legacy-model-guide", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully"}
{"long_name": "command", "short_name": "command", "longcode": "COM", "model_family": "Cohere", "model_provider": "Cohere", "model_provider_type": "Commerical", "model_type": "Instruct", "default_params": {"temperature": "0.3"}, "selected_params": {"temperature": "1.0", "max_tokens": "256", "top_k": "5", "top_p": "0.9"}, "header": "You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://docs.cohere.com/docs/models", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully"}
{"long_name": "command-light", "short_name": "command-light", "longcode": "COML", "model_family": "Cohere", "model_provider": "Cohere", "model_provider_type": "Commerical", "model_type": "Instruct", "default_params": {"temperature": "0.3"}, "selected_params": {"temperature": "1.0", "max_tokens": "256", "top_k": "5", "top_p": "0.9"}, "header": "You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://docs.cohere.com/docs/models", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully"}
{"long_name": "command-nightly", "short_name": "command-nightly", "longcode": "COMN", "model_family": "Cohere", "model_provider": "Cohere", "model_provider_type": "Commerical", "model_type": "Instruct", "default_params": {"temperature": "0.3"}, "selected_params": {"temperature": "1.0", "max_tokens": "256", "top_k": "5", "top_p": "0.9"}, "header": "You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://docs.cohere.com/docs/models", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully"}
{"long_name": "google/flan-t5-xxl", "short_name": "flan-t5-xxl", "longcode": "FLAN", "model_family": "Google", "model_provider": "HuggingFace-API", "model_provider_type": "Open Access", "model_type": "Instruct", "default_params": {"temperature": "1.0", "top_p": "1.0", "top_k": "50", "min_tokens": "0", "max_tokens": "20", "is_llama": 0, "is_vicuna": 0, "is_falcon": 0, "is_pythia": 0, "is_guanaco": 0, "is_zephyr": 0}, "selected_params": {"temperature": "1.0", "top_p": "0.9", "top_k": "50", "min_tokens": "10", "max_tokens": "200", "is_llama": 0, "is_vicuna": 0, "is_falcon": 0, "is_pythia": 0, "is_guanaco": 0, "is_zephyr": 0}, "header": "You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://huggingface.co/google/flan-t5-xxl", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully"}
{"long_name": "gpt-3.5-turbo", "short_name": "gpt-3.5-turbo", "longcode": "GP3*", "model_family": "OpenAI", "model_provider": "OpenAI", "model_provider_type": "Commerical", "model_type": "Chat", "default_params": {"temperature": "1.0", "top_p": "1.0", "presence_penalty": "0.0", "frequency_penalty": "0.0"}, "selected_params": {"temperature": "1.0", "top_p": "1.0", "presence_penalty": "0.0", "frequency_penalty": "0.0", "max_tokens": "256"}, "header": "You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://platform.openai.com/docs/models/gpt-3-5 ", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully"}
{"long_name": "gpt-4", "short_name": "gpt-4", "longcode": "GPT4", "model_family": "OpenAI", "model_provider": "OpenAI", "model_provider_type": "Commerical", "model_type": "Chat", "default_params": {"temperature": "1.0", "top_p": "1.0", "presence_penalty": "0.0", "frequency_penalty": "0.0"}, "selected_params": {"temperature": "1.0", "top_p": "1.0", "presence_penalty": "0.0", "frequency_penalty": "0.0", "max_tokens": "256"}, "header": "You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully"}
{"long_name": "gpt-4-1106-preview", "short_name": "gpt-4-turbo", "longcode": "GPT4*", "model_family": "OpenAI", "model_provider": "OpenAI", "model_provider_type": "Commerical", "model_type": "Chat", "default_params": {"temperature": "1.0", "top_p": "1.0", "presence_penalty": "0.0", "frequency_penalty": "0.0"}, "selected_params": {"temperature": "1.0", "top_p": "1.0", "presence_penalty": "0.0", "frequency_penalty": "0.0", "max_tokens": "256"}, "header": "You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully"}
{"long_name": "HuggingFaceH4/zephyr-7b-beta", "short_name": "zephyr-7b-beta", "longcode": "ZEPH", "model_family": "HuggingFace", "model_provider": "HuggingFace-API", "model_provider_type": "Open Access", "model_type": "Chat", "default_params": {"temperature": "1.0", "top_p": "1.0", "top_k": "50", "min_tokens": "0", "max_tokens": "20", "is_llama": 0, "is_vicuna": 0, "is_falcon": 0, "is_pythia": 0, "is_guanaco": 0, "is_zephyr": 0}, "selected_params": {"temperature": "1.0", "top_p": "0.9", "top_k": "50", "min_tokens": "10", "max_tokens": "200", "is_llama": 0, "is_vicuna": 0, "is_falcon": 0, "is_pythia": 0, "is_guanaco": 0, "is_zephyr": 1}, "header": "You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://huggingface.co/HuggingFaceH4/zephyr-7b-beta", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed"}
{"long_name": "luminous-extended-control", "short_name": "luminous-extended-control", "longcode": "LUMX", "model_family": "Aleph Alpha", "model_provider": "Aleph", "model_provider_type": "Commerical", "model_type": "Instruct", "default_params": {"temperature": "0.0", "top_p": "0.0", "max_tokens": "64", "top_k": "0", "presence_penalty": "0.0", "frequency_penalty": "0.0"}, "selected_params": {"temperature": "1.0", "top_p": "0.0", "max_tokens": "256", "top_k": "0", "presence_penalty": "0.0", "frequency_penalty": "0.0"}, "header": "You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://docs.aleph-alpha.com/docs/introduction/luminous/", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully"}
{"long_name": "luminous-supreme-control", "short_name": "luminous-supreme-control", "longcode": "LUMS", "model_family": "Aleph Alpha", "model_provider": "Aleph", "model_provider_type": "Commerical", "model_type": "Instruct", "default_params": {"temperature": "0.0", "top_p": "0.0", "max_tokens": "64", "top_k": "0", "presence_penalty": "0.0", "frequency_penalty": "0.0"}, "selected_params": {"temperature": "1.0", "top_p": "0.0", "max_tokens": "256", "top_k": "0", "presence_penalty": "0.0", "frequency_penalty": "0.0"}, "header": "You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://docs.aleph-alpha.com/docs/introduction/luminous/", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully"}
{"long_name": "meta-llama/Llama-2-13b-chat-hf", "short_name": "llama-2-13b-chat", "longcode": "LL13", "model_family": "Meta", "model_provider": "HuggingFace-API", "model_provider_type": "Open Access", "model_type": "Chat", "default_params": {"temperature": "0.6", "top_p": "0.9", "top_k": "50", "min_tokens": "0", "max_tokens": "None", "is_llama": 1, "is_vicuna": 0, "is_falcon": 0, "is_pythia": 0, "is_guanaco": 0, "is_zephyr": 0}, "selected_params": {"temperature": "1.0", "top_p": "0.9", "top_k": "50", "min_tokens": "10", "max_tokens": "200", "is_llama": 1, "is_vicuna": 0, "is_falcon": 0, "is_pythia": 0, "is_guanaco": 0, "is_zephyr": 0}, "header": "You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://huggingface.co/meta-llama/Llama-2-13b-chat-hf", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully (but fails intermittently)"}
{"long_name": "meta-llama/Llama-2-70b-chat-hf", "short_name": "llama-2-70b-chat", "longcode": "LL70", "model_family": "Meta", "model_provider": "HuggingFace-API", "model_provider_type": "Open Access", "model_type": "Chat", "default_params": {"temperature": "0.6", "top_p": "0.9", "top_k": "50", "min_tokens": "0", "max_tokens": "None", "is_llama": 1, "is_vicuna": 0, "is_falcon": 0, "is_pythia": 0, "is_guanaco": 0, "is_zephyr": 0}, "selected_params": {"temperature": "1.0", "top_p": "0.9", "top_k": "50", "min_tokens": "10", "max_tokens": "200", "is_llama": 1, "is_vicuna": 0, "is_falcon": 0, "is_pythia": 0, "is_guanaco": 0, "is_zephyr": 0}, "header": "You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://huggingface.co/meta-llama/Llama-2-70b-chat-hf", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully"}
{"long_name": "meta-llama/Llama-2-7b-chat-hf", "short_name": "llama-2-7b-chat", "longcode": "LL7", "model_family": "Meta", "model_provider": "HuggingFace-API", "model_provider_type": "Open Access", "model_type": "Chat", "default_params": {"temperature": "0.6", "top_p": "0.9", "top_k": "50", "min_tokens": "0", "max_tokens": "None", "is_llama": 1, "is_vicuna": 0, "is_falcon": 0, "is_pythia": 0, "is_guanaco": 0, "is_zephyr": 0}, "selected_params": {"temperature": "1.0", "top_p": "0.9", "top_k": "50", "min_tokens": "10", "max_tokens": "200", "is_llama": 1, "is_vicuna": 0, "is_falcon": 0, "is_pythia": 0, "is_guanaco": 0, "is_zephyr": 0}, "header": "You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://huggingface.co/meta-llama/Llama-2-7b-chat-hf", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully"}
{"long_name": "mistralai/Mistral-7B-Instruct-v0.1", "short_name": "mistral-7b-instruct", "longcode": "MIST", "model_family": "Mistral", "model_provider": "HuggingFace-API", "model_provider_type": "Open Access", "model_type": "Instruct", "default_params": {"temperature": "1.0", "top_p": "1.0", "top_k": "50", "min_tokens": "0", "max_tokens": "20", "is_llama": 0, "is_vicuna": 0, "is_falcon": 0, "is_pythia": 0, "is_guanaco": 0, "is_zephyr": 0}, "selected_params": {"temperature": "1.0", "top_p": "0.9", "top_k": "50", "min_tokens": "10", "max_tokens": "200", "is_llama": 1, "is_vicuna": 0, "is_falcon": 0, "is_pythia": 0, "is_guanaco": 0, "is_zephyr": 0}, "header": "You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully"}
{"long_name": "models/chat-bison-001", "short_name": "palm-2", "longcode": "PALM", "model_family": "Google", "model_provider": "Google", "model_provider_type": "Commerical", "model_type": "Chat", "default_params": {"temperature": "0.0", "top_p": "0.9", "max_tokens": "1024", "top_k": "40"}, "selected_params": {"temperature": "1.0", "top_p": "0.9", "max_tokens": "256", "top_k": "40"}, "header": "You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://ai.google.dev/palm_docs", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully"}
{"long_name": "OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5", "short_name": "pythia-12b", "longcode": "PYTH", "model_family": "Other OA", "model_provider": "HuggingFace-API", "model_provider_type": "Open Access", "model_type": "Chat", "default_params": {"temperature": "1.0", "top_p": "1.0", "top_k": "50", "min_tokens": "0", "max_tokens": "20", "is_llama": 0, "is_vicuna": 0, "is_falcon": 0, "is_pythia": 0, "is_guanaco": 0, "is_zephyr": 0}, "selected_params": {"temperature": "1.0", "top_p": "0.9", "top_k": "50", "min_tokens": "10", "max_tokens": "200", "is_llama": 0, "is_vicuna": 0, "is_falcon": 0, "is_pythia": 1, "is_guanaco": 0, "is_zephyr": 0}, "header": "You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully"}
{"long_name": "tiiuae/falcon-7b-instruct", "short_name": "falcon-7b-instruct", "longcode": "FAL7", "model_family": "Other OA", "model_provider": "HuggingFace-API", "model_provider_type": "Open Access", "model_type": "Instruct", "default_params": {"temperature": "1.0", "top_p": "1.0", "top_k": "50", "min_tokens": "0", "max_tokens": "20", "is_llama": 0, "is_vicuna": 0, "is_falcon": 0, "is_pythia": 0, "is_guanaco": 0, "is_zephyr": 0}, "selected_params": {"temperature": "1.0", "top_p": "0.9", "top_k": "50", "min_tokens": "10", "max_tokens": "200", "is_llama": 0, "is_vicuna": 0, "is_falcon": 1, "is_pythia": 0, "is_guanaco": 0, "is_zephyr": 0}, "header": "You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://huggingface.co/tiiuae/falcon-7b-instruct", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully"}
{"long_name": "timdettmers/guanaco-33b-merged", "short_name": "guanaco-33b", "longcode": "GUAN", "model_family": "Other OA", "model_provider": "HuggingFace-API", "model_provider_type": "Open Access", "model_type": "Instruct", "default_params": {"temperature": "1.0", "top_p": "1.0", "top_k": "50", "min_tokens": "0", "max_tokens": "20", "is_llama": 0, "is_vicuna": 0, "is_falcon": 0, "is_pythia": 0, "is_guanaco": 0, "is_zephyr": 0}, "selected_params": {"temperature": "1.0", "top_p": "0.9", "top_k": "50", "min_tokens": "10", "max_tokens": "200", "is_llama": 0, "is_vicuna": 0, "is_falcon": 0, "is_pythia": 0, "is_guanaco": 1, "is_zephyr": 0}, "header": "You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit.", "link": "https://huggingface.co/timdettmers/guanaco-33b-merged", "in_yaml": "Yes", "params_tested": "Yes", "endpoint_live": "Yes", "status": "Deployed Successfully"}
