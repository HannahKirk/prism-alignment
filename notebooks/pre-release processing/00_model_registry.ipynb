{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Registry\n",
    "This notebook keeps a log of model parameters for the conversations interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from src.utils.helper_funcs import find_project_root, save_as_jsonl\n",
    "\n",
    "PROJECT_ROOT = find_project_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_info(\n",
    "    OUTPUT_PATH, df, provider, model_id, base_updates, selected_params_overrides=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Updates the model information for a given model_id in the dataframe and logs the changes,\n",
    "    including setting selected_params as a copy of default_params with optional overrides.\n",
    "\n",
    "    Parameters:\n",
    "    OUTPUT_PATH (str): The path to save the logs.\n",
    "    df (pd.DataFrame): The dataframe containing the models.\n",
    "    model_id (str): The model_id of the model to update.\n",
    "    base_updates (dict): A dictionary with base updates to apply.\n",
    "    selected_params_overrides (dict, optional): Dictionary of overrides for selected_params.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The updated dataframe.\n",
    "    \"\"\"\n",
    "    # Serialize the 'default_params' dictionary to a JSON string if it's a dictionary\n",
    "    if \"default_params\" in base_updates and isinstance(\n",
    "        base_updates[\"default_params\"], dict\n",
    "    ):\n",
    "        base_updates[\"default_params\"] = json.dumps(base_updates[\"default_params\"])\n",
    "\n",
    "    # Initialize changes log\n",
    "    changes_log = [f\"#PROVIDER = {provider}\\n#MODEL_ID = {model_id}\"]\n",
    "\n",
    "    # Check if the model_id exists in the dataframe\n",
    "    if model_id not in df[\"long_name\"].values:\n",
    "        changes_log.append(f\"No entry found for model_id {model_id}. No updates made.\")\n",
    "        # Append the message to the log file and exit\n",
    "        append_to_log(OUTPUT_PATH, changes_log)\n",
    "        return df\n",
    "\n",
    "    # Retrieve the index for the row that matches the model_id\n",
    "    index = df.index[df[\"long_name\"] == model_id].tolist()\n",
    "    if not index:\n",
    "        changes_log.append(f\"No entry found for model_id {model_id}. No updates made.\")\n",
    "        # Append the message to the log file and exit\n",
    "        append_to_log(OUTPUT_PATH, changes_log)\n",
    "        return df\n",
    "    index = index[0]\n",
    "\n",
    "    # Prepare selected_params by copying default_params if available\n",
    "    if \"default_params\" in base_updates:\n",
    "        selected_params = json.loads(base_updates[\"default_params\"])\n",
    "\n",
    "        # Apply any specified overrides to selected_params\n",
    "        if selected_params_overrides:\n",
    "            for key, value in selected_params_overrides.items():\n",
    "                selected_params[key] = value\n",
    "                changes_log.append(f\"Overridden '{key}' in selected_params: {value}\")\n",
    "\n",
    "        # Serialize the selected_params again after modification\n",
    "        base_updates[\"selected_params\"] = json.dumps(selected_params)\n",
    "\n",
    "    # Update the dataframe with the new values\n",
    "    for column, new_value in base_updates.items():\n",
    "        old_value = df.at[index, column]\n",
    "        if old_value != new_value:\n",
    "            changes_log.append(f\"Updated '{column}' from {old_value} to {new_value}\")\n",
    "            df.at[index, column] = new_value\n",
    "\n",
    "    # Append the changes log to the log file\n",
    "    append_to_log(OUTPUT_PATH, changes_log)\n",
    "\n",
    "    print(f\"Updates for model_id {model_id} appended to the log file.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def append_to_log(OUTPUT_PATH, changes_log):\n",
    "    \"\"\"\n",
    "    Appends the changes log to a single log file.\n",
    "\n",
    "    Parameters:\n",
    "    OUTPUT_PATH (str): The path where the log file is saved.\n",
    "    changes_log (list): The list of log entries to append to the file.\n",
    "    \"\"\"\n",
    "    log_filename = f\"{OUTPUT_PATH}/model_registry_update_log.txt\"\n",
    "    with open(log_filename, \"a\") as log_file:\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_file.write(f\"\\nUpdate timestamp: {timestamp}\\n\")\n",
    "        for change in changes_log:\n",
    "            log_file.write(change + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_ids_by_provider(df, provider_name):\n",
    "    \"\"\"\n",
    "    Prints all the model_ids for a given provider name.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataframe containing the models.\n",
    "    provider_name (str): The name of the provider.\n",
    "    \"\"\"\n",
    "    # Filter the dataframe for rows that match the provider name\n",
    "    provider_df = df[df[\"provider\"].str.lower() == provider_name.lower()]\n",
    "\n",
    "    # Get the model_ids\n",
    "    model_ids = provider_df[\"long_name\"].tolist()\n",
    "    model_names = provider_df[\"short_name\"].tolist()\n",
    "    model_links = provider_df[\"link\"].tolist()\n",
    "\n",
    "    # Print the model_ids\n",
    "    print(f\"<Model Name>:<Model ID> for provider {provider_name}:\")\n",
    "    for model_id, model_name, link in zip(model_ids, model_names, model_links):\n",
    "        print(f\"{model_name}:{model_id}, see: {link}\")\n",
    "\n",
    "    return model_ids\n",
    "\n",
    "\n",
    "def print_model_summary_from_df_row(model_df, model_id_to_print):\n",
    "    \"\"\"\n",
    "    Prints the row in the dataframe for a given model_id, printing every column that is not NA.\n",
    "\n",
    "    Parameters:\n",
    "    model_df (pd.DataFrame): The dataframe containing the models.\n",
    "    model_id_to_print (str): The model_id of the model to print.\n",
    "    \"\"\"\n",
    "    # Filter the dataframe for rows that match the model_id\n",
    "    model_row = model_df[model_df[\"long_name\"] == model_id_to_print]\n",
    "\n",
    "    # Print the row\n",
    "    print(f\"\\n\\n**Model summary for model_id {model_id_to_print}**\\n\")\n",
    "    for col in model_row.columns:\n",
    "        if not pd.isna(model_row[col].values[0]):\n",
    "            print(f\"{col}: {model_row[col].values[0]}\")\n",
    "    print(\"[end of summary]\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['long_name', 'short_name', 'longcode', 'model_family', 'provider',\n",
      "       'provider_type', 'model_type', 'default_params', 'selected_params',\n",
      "       'header', 'footer', 'cost_per_token', 'tokens_per_completion',\n",
      "       'cost_per_completion', 'cost_per_100_completions', 'link', 'in_yaml',\n",
      "       'params_tested', 'endpoint_live', 'status'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "PATH = f\"{PROJECT_ROOT}/data/interim\"\n",
    "model_df = pd.read_csv(f\"{PATH}/models.csv\")\n",
    "print(model_df.columns)\n",
    "\n",
    "string_cols = [\n",
    "    \"long_name\",\n",
    "    \"short_name\" \"model_family\",\n",
    "    \"provider\",\n",
    "    \"provider_type\",\n",
    "    \"model_type\",\n",
    "    \"header\",\n",
    "    \"footer\",\n",
    "    \"link\",\n",
    "    \"in yaml\",\n",
    "    \"params_tested\",\n",
    "    \"status\",\n",
    "]\n",
    "\n",
    "float_cols = [\n",
    "    \"cost_per_token\",\n",
    "    \"tokens_per_completion\",\n",
    "    \"cost_per_completion\",\n",
    "    \"cost_per_100_completions\",\n",
    "]\n",
    "\n",
    "json_cols = [\"default_params\", \"selected_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anthropic' 'Cohere' 'HuggingFace-API' 'OpenAI' 'Aleph' 'Google']\n"
     ]
    }
   ],
   "source": [
    "# We'll loop through providers\n",
    "print(model_df[\"provider\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Information Across Providers\n",
    "\n",
    "Note:\n",
    "```\n",
    "prompt = f\"{head_template} {prompt} {foot_template}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_HEADER = \"You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.\"\n",
    "BASE_HEADER_INSTRUCT = \"You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit.\"\n",
    "BASE_FOOTER = \"\"\n",
    "MAX_TOKENS = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provider: OpenAI\n",
    "\n",
    "OpenAI provides default params here: https://platform.openai.com/docs/api-reference/chat/create. Sharing defaults across models.\n",
    "\n",
    "6/11/2023: GPT-4-turbo announced (gpt-4-1106-preview) and added to registry.\n",
    "\n",
    "The chat template is provided: https://platform.openai.com/docs/guides/text-generation/chat-completions-api\n",
    "\n",
    "```\n",
    "messages = [{\"role\": \"user\",\n",
    "            \"content\": \"<prompt>\"},\n",
    "            {\"role\": \"assistant\",\n",
    "            \"content\": \"<reply>\"}\n",
    "            ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Model Name>:<Model ID> for provider OpenAI:\n",
      "gpt-3.5-turbo:gpt-3.5-turbo, see: https://platform.openai.com/docs/models/gpt-3-5 \n",
      "gpt-4:gpt-4, see: https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\n",
      "gpt-4-turbo:gpt-4-1106-preview, see: https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\n"
     ]
    }
   ],
   "source": [
    "provider = \"OpenAI\"\n",
    "model_ids = print_model_ids_by_provider(model_df, provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates for model_id gpt-3.5-turbo appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id gpt-3.5-turbo**\n",
      "\n",
      "long_name: gpt-3.5-turbo\n",
      "short_name: gpt-3.5-turbo\n",
      "longcode: GP3*\n",
      "model_family: OpenAI\n",
      "provider: OpenAI\n",
      "provider_type: Commerical\n",
      "model_type: Chat\n",
      "default_params: {\"temperature\": 1.0, \"top_p\": 1.0, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0}\n",
      "selected_params: {\"temperature\": 1.0, \"top_p\": 1.0, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0, \"max_tokens\": 256}\n",
      "header: You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 1.75e-06\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.00013125\n",
      "cost_per_100_completions: 0.0013125\n",
      "link: https://platform.openai.com/docs/models/gpt-3-5 \n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully\n",
      "[end of summary]\n",
      "\n",
      "Updates for model_id gpt-4 appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id gpt-4**\n",
      "\n",
      "long_name: gpt-4\n",
      "short_name: gpt-4\n",
      "longcode: GPT4\n",
      "model_family: OpenAI\n",
      "provider: OpenAI\n",
      "provider_type: Commerical\n",
      "model_type: Chat\n",
      "default_params: {\"temperature\": 1.0, \"top_p\": 1.0, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0}\n",
      "selected_params: {\"temperature\": 1.0, \"top_p\": 1.0, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0, \"max_tokens\": 256}\n",
      "header: You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 4e-05\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.003\n",
      "cost_per_100_completions: 0.03\n",
      "link: https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully\n",
      "[end of summary]\n",
      "\n",
      "Updates for model_id gpt-4-1106-preview appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id gpt-4-1106-preview**\n",
      "\n",
      "long_name: gpt-4-1106-preview\n",
      "short_name: gpt-4-turbo\n",
      "longcode: GPT4*\n",
      "model_family: OpenAI\n",
      "provider: OpenAI\n",
      "provider_type: Commerical\n",
      "model_type: Chat\n",
      "default_params: {\"temperature\": 1.0, \"top_p\": 1.0, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0}\n",
      "selected_params: {\"temperature\": 1.0, \"top_p\": 1.0, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0, \"max_tokens\": 256}\n",
      "header: You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 2e-05\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.0015\n",
      "cost_per_100_completions: 0.015\n",
      "link: https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully\n",
      "[end of summary]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gh/sygthfxn2tv1f1d1d_8s8kvc0000gn/T/ipykernel_65500/3986135523.py:61: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[index, column] = new_value\n"
     ]
    }
   ],
   "source": [
    "for model_id_to_update in model_ids:\n",
    "    base_updates = {\n",
    "        \"header\": BASE_HEADER,\n",
    "        \"footer\": BASE_FOOTER,\n",
    "        \"default_params\": {\n",
    "            \"temperature\": 1.0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"presence_penalty\": 0.0,\n",
    "            \"frequency_penalty\": 0.0,\n",
    "        },\n",
    "        \"in_yaml\": \"Yes\",\n",
    "        \"params_tested\": \"Yes\",\n",
    "        \"status\": \"Deployed Successfully\",\n",
    "    }\n",
    "\n",
    "    selected_params_overrides = {\n",
    "        \"max_tokens\": MAX_TOKENS,  # Override the max_tokens\n",
    "    }\n",
    "\n",
    "    model_df = update_model_info(\n",
    "        PATH,\n",
    "        model_df,\n",
    "        provider,\n",
    "        model_id_to_update,\n",
    "        base_updates,\n",
    "        selected_params_overrides,\n",
    "    )\n",
    "\n",
    "    print_model_summary_from_df_row(model_df, model_id_to_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic\n",
    "\n",
    "Anthropic has defaults for completions which can be found here: https://docs.anthropic.com/claude/reference/complete_post. Sharing defaults across all models. Temperature of 1.0 seems to produce responses that are very similar but is the max of permitted range. [edit: actually only for responses like \"what's your favoruite food/person/word\"..., some effect of RLHF?]\n",
    "\n",
    "Chat format is provided: https://docs.anthropic.com/claude/reference/complete_post\n",
    "```\n",
    "final_prompt = (\n",
    "    f\"\\n\\nHuman: <prompt> \\n\\nAssistant:\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Model Name>:<Model ID> for provider Anthropic:\n",
      "claude-2:claude-2, see: https://docs.anthropic.com/claude/docs/legacy-model-guide\n",
      "claude-2.1:claude-2.1, see: https://docs.anthropic.com/claude/docs/legacy-model-guide\n",
      "claude-instant-1:claude-instant-1, see: https://docs.anthropic.com/claude/docs/legacy-model-guide\n"
     ]
    }
   ],
   "source": [
    "provider = \"Anthropic\"\n",
    "model_ids = print_model_ids_by_provider(model_df, provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates for model_id claude-2 appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id claude-2**\n",
      "\n",
      "long_name: claude-2\n",
      "short_name: claude-2\n",
      "longcode: CL2\n",
      "model_family: Anthropic\n",
      "provider: Anthropic\n",
      "provider_type: Commerical\n",
      "model_type: Chat\n",
      "default_params: {\"temperature\": 1.0, \"top_p\": 0.7, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0, \"max_tokens\": 256, \"top_k\": 5}\n",
      "selected_params: {\"temperature\": 1.0, \"top_p\": 0.7, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0, \"max_tokens\": 256, \"top_k\": 5}\n",
      "header: You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 0.0\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.0\n",
      "cost_per_100_completions: 0.0\n",
      "link: https://docs.anthropic.com/claude/docs/legacy-model-guide\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully\n",
      "[end of summary]\n",
      "\n",
      "Updates for model_id claude-2.1 appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id claude-2.1**\n",
      "\n",
      "long_name: claude-2.1\n",
      "short_name: claude-2.1\n",
      "longcode: CL2*\n",
      "model_family: Anthropic\n",
      "provider: Anthropic\n",
      "provider_type: Commerical\n",
      "model_type: Chat\n",
      "default_params: {\"temperature\": 1.0, \"top_p\": 0.7, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0, \"max_tokens\": 256, \"top_k\": 5}\n",
      "selected_params: {\"temperature\": 1.0, \"top_p\": 0.7, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0, \"max_tokens\": 256, \"top_k\": 5}\n",
      "header: You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 0.0\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.0\n",
      "cost_per_100_completions: 0.0\n",
      "link: https://docs.anthropic.com/claude/docs/legacy-model-guide\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully\n",
      "[end of summary]\n",
      "\n",
      "Updates for model_id claude-instant-1 appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id claude-instant-1**\n",
      "\n",
      "long_name: claude-instant-1\n",
      "short_name: claude-instant-1\n",
      "longcode: CL1\n",
      "model_family: Anthropic\n",
      "provider: Anthropic\n",
      "provider_type: Commerical\n",
      "model_type: Chat\n",
      "default_params: {\"temperature\": 1.0, \"top_p\": 0.7, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0, \"max_tokens\": 256, \"top_k\": 5}\n",
      "selected_params: {\"temperature\": 1.0, \"top_p\": 0.7, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0, \"max_tokens\": 256, \"top_k\": 5}\n",
      "header: You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 0.0\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.0\n",
      "cost_per_100_completions: 0.0\n",
      "link: https://docs.anthropic.com/claude/docs/legacy-model-guide\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully\n",
      "[end of summary]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_id_to_update in model_ids:\n",
    "    base_updates = {\n",
    "        \"header\": BASE_HEADER,\n",
    "        \"footer\": BASE_FOOTER,\n",
    "        \"default_params\": {\n",
    "            \"temperature\": 1.0,\n",
    "            \"top_p\": 0.7,\n",
    "            \"presence_penalty\": 0.0,\n",
    "            \"frequency_penalty\": 0.0,\n",
    "            \"max_tokens\": 256,\n",
    "            \"top_k\": 5,\n",
    "        },\n",
    "        \"in_yaml\": \"Yes\",\n",
    "        \"params_tested\": \"Yes\",\n",
    "        \"status\": \"Deployed Successfully\",\n",
    "    }\n",
    "\n",
    "    selected_params_overrides = {\n",
    "        \"max_tokens\": MAX_TOKENS,  # Override the max_tokens\n",
    "    }\n",
    "\n",
    "    model_df = update_model_info(\n",
    "        PATH,\n",
    "        model_df,\n",
    "        provider,\n",
    "        model_id_to_update,\n",
    "        base_updates,\n",
    "        selected_params_overrides,\n",
    "    )\n",
    "\n",
    "    print_model_summary_from_df_row(model_df, model_id_to_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohere\n",
    "\n",
    "Cohere provides default params that can be found here: https://docs.cohere.com/reference/generate. Sharing defaults across models. Note you can actually only set temperature in the chat API? (https://docs.cohere.com/reference/chat)\n",
    "\n",
    "Default of 0.3 in the chat API was too low. Increased to 1.0.\n",
    "\n",
    "Format of chat history is provided: https://docs.cohere.com/reference/chat\n",
    "\n",
    "```\n",
    "chat_history=[\n",
    "{\"role\": \"USER\", \"message\": \"<prompt>\"},\n",
    "{\"role\": \"CHATBOT\", \"message\": \"<reply>\"}\n",
    "],\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Model Name>:<Model ID> for provider Cohere:\n",
      "command:command, see: https://docs.cohere.com/docs/models\n",
      "command-light:command-light, see: https://docs.cohere.com/docs/models\n",
      "command-nightly:command-nightly, see: https://docs.cohere.com/docs/models\n"
     ]
    }
   ],
   "source": [
    "provider = \"Cohere\"\n",
    "model_ids = print_model_ids_by_provider(model_df, provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates for model_id command appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id command**\n",
      "\n",
      "long_name: command\n",
      "short_name: command\n",
      "longcode: COM\n",
      "model_family: Cohere\n",
      "provider: Cohere\n",
      "provider_type: Commerical\n",
      "model_type: Instruct\n",
      "default_params: {\"temperature\": 0.3}\n",
      "selected_params: {\"temperature\": 1.0, \"max_tokens\": 256, \"top_k\": 5, \"top_p\": 0.9}\n",
      "header: You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 2e-06\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.00015\n",
      "cost_per_100_completions: 0.0015\n",
      "link: https://docs.cohere.com/docs/models\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully\n",
      "[end of summary]\n",
      "\n",
      "Updates for model_id command-light appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id command-light**\n",
      "\n",
      "long_name: command-light\n",
      "short_name: command-light\n",
      "longcode: COML\n",
      "model_family: Cohere\n",
      "provider: Cohere\n",
      "provider_type: Commerical\n",
      "model_type: Instruct\n",
      "default_params: {\"temperature\": 0.3}\n",
      "selected_params: {\"temperature\": 1.0, \"max_tokens\": 256, \"top_k\": 5, \"top_p\": 0.9}\n",
      "header: You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 2e-06\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.00015\n",
      "cost_per_100_completions: 0.0015\n",
      "link: https://docs.cohere.com/docs/models\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully\n",
      "[end of summary]\n",
      "\n",
      "Updates for model_id command-nightly appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id command-nightly**\n",
      "\n",
      "long_name: command-nightly\n",
      "short_name: command-nightly\n",
      "longcode: COMN\n",
      "model_family: Cohere\n",
      "provider: Cohere\n",
      "provider_type: Commerical\n",
      "model_type: Instruct\n",
      "default_params: {\"temperature\": 0.3}\n",
      "selected_params: {\"temperature\": 1.0, \"max_tokens\": 256, \"top_k\": 5, \"top_p\": 0.9}\n",
      "header: You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 2e-06\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.00015\n",
      "cost_per_100_completions: 0.0015\n",
      "link: https://docs.cohere.com/docs/models\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully\n",
      "[end of summary]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_id_to_update in model_ids:\n",
    "    base_updates = {\n",
    "        \"header\": BASE_HEADER,\n",
    "        \"footer\": BASE_FOOTER,\n",
    "        \"default_params\": {\n",
    "            \"temperature\": 0.3,\n",
    "            # \"top_p\": 0, # These are all defaults from generate not chat\n",
    "            # \"presence_penalty\": 0.0,\n",
    "            # \"frequency_penalty\": 0.0,\n",
    "            # \"max_tokens\": None,\n",
    "            # \"top_k\": 0,\n",
    "        },\n",
    "        \"in_yaml\": \"Yes\",\n",
    "        \"params_tested\": \"Yes\",\n",
    "        \"status\": \"Deployed Successfully\",\n",
    "    }\n",
    "\n",
    "    selected_params_overrides = {\n",
    "        \"max_tokens\": MAX_TOKENS,  # Override the max_tokens\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_k\": 5,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "\n",
    "    model_df = update_model_info(\n",
    "        PATH,\n",
    "        model_df,\n",
    "        provider,\n",
    "        model_id_to_update,\n",
    "        base_updates,\n",
    "        selected_params_overrides,\n",
    "    )\n",
    "\n",
    "    print_model_summary_from_df_row(model_df, model_id_to_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google\n",
    "\n",
    "Google does not provide default params here: https://developers.generativeai.google/api/python/google/generativeai/generate_text.\n",
    "\n",
    "There are some mentioned here: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-chat, treating those as default parameters. Overriding temp to 1.0 because 0.0 is deterministic.\n",
    "\n",
    "Chat format is provided:\n",
    "\n",
    "```\n",
    "[{\n",
    "  \"author\": \"user\",\n",
    "  \"content\": \"user message\"\n",
    "}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Model Name>:<Model ID> for provider Google:\n",
      "palm-2:models/chat-bison-001, see: https://ai.google.dev/palm_docs\n"
     ]
    }
   ],
   "source": [
    "provider = \"Google\"\n",
    "model_ids = print_model_ids_by_provider(model_df, provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates for model_id models/chat-bison-001 appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id models/chat-bison-001**\n",
      "\n",
      "long_name: models/chat-bison-001\n",
      "short_name: palm-2\n",
      "longcode: PALM\n",
      "model_family: Google\n",
      "provider: Google\n",
      "provider_type: Commerical\n",
      "model_type: Chat\n",
      "default_params: {\"temperature\": 0.0, \"top_p\": 0.95, \"max_tokens\": 1024, \"top_k\": 40}\n",
      "selected_params: {\"temperature\": 1.0, \"top_p\": 0.95, \"max_tokens\": 256, \"top_k\": 40}\n",
      "header: You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 0.0\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.0\n",
      "cost_per_100_completions: 0.0\n",
      "link: https://ai.google.dev/palm_docs\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully\n",
      "[end of summary]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_id_to_update in model_ids:\n",
    "    base_updates = {\n",
    "        \"header\": BASE_HEADER,\n",
    "        \"footer\": BASE_FOOTER,\n",
    "        \"default_params\": {\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_p\": 0.95,\n",
    "            \"max_tokens\": 1024,\n",
    "            \"top_k\": 40,\n",
    "        },\n",
    "        \"in_yaml\": \"Yes\",\n",
    "        \"params_tested\": \"Yes\",\n",
    "        \"status\": \"Deployed Successfully\",\n",
    "    }\n",
    "\n",
    "    selected_params_overrides = {\n",
    "        \"max_tokens\": MAX_TOKENS,  # Override the max_tokens\n",
    "        \"temperature\": 1.0,  # Override temp because it's deterministic\n",
    "    }\n",
    "\n",
    "    model_df = update_model_info(\n",
    "        PATH,\n",
    "        model_df,\n",
    "        provider,\n",
    "        model_id_to_update,\n",
    "        base_updates,\n",
    "        selected_params_overrides,\n",
    "    )\n",
    "\n",
    "    print_model_summary_from_df_row(model_df, model_id_to_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aleph Alpha\n",
    "\n",
    "Aleph Alpha provides default parameters: https://aleph-alpha-client.readthedocs.io/en/latest/_modules/aleph_alpha_client/completion.html. Temperature = 0.0 is determinstic so we increase it to 1.0.\n",
    "\n",
    "Format is provided. Note not explictly chat optimised: https://docs.aleph-alpha.com/docs/introduction/zero_shot_control/.\n",
    "\n",
    "```\n",
    "### Instruction:\n",
    "INPUT YOUR INSTRUCTION HERE\n",
    "\n",
    "### Input:\n",
    "YOUR INPUT\n",
    "\n",
    "### Response:\n",
    "COMPLETION OF THE MODEL\n",
    "```\n",
    "\n",
    "Note we spoke to the team at Aleph Alpha and they helped us with some changes to encourage better conversational behaviour\n",
    "\n",
    "```\n",
    "prompt = f\"\"\"### Instruction \\n{head_template}\n",
    "\\n###Input \\nLast user message: {prompt} \\n\\n### Response: \\nAssistant:\"\"\"\n",
    "```\n",
    "\n",
    "And recommended an update to the prompt header:\n",
    "```\n",
    "BASE_HEADER_INSTRUCT = \"You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit.\"\n",
    "```\n",
    "\n",
    "We subsequently decided to use this for all instruct-tuned but not chat-tuned models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Model Name>:<Model ID> for provider Aleph:\n",
      "luminous-extended-control:luminous-extended-control, see: https://docs.aleph-alpha.com/docs/introduction/luminous/\n",
      "luminous-supreme-control:luminous-supreme-control, see: https://docs.aleph-alpha.com/docs/introduction/luminous/\n"
     ]
    }
   ],
   "source": [
    "provider = \"Aleph\"\n",
    "model_ids = print_model_ids_by_provider(model_df, provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates for model_id luminous-extended-control appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id luminous-extended-control**\n",
      "\n",
      "long_name: luminous-extended-control\n",
      "short_name: luminous-extended-control\n",
      "longcode: LUMX\n",
      "model_family: Aleph Alpha\n",
      "provider: Aleph\n",
      "provider_type: Commerical\n",
      "model_type: Instruct\n",
      "default_params: {\"temperature\": 0.0, \"top_p\": 0.0, \"max_tokens\": 64, \"top_k\": 0, \"presence_penalty\": 0, \"frequency_penalty\": 0}\n",
      "selected_params: {\"temperature\": 1.0, \"top_p\": 0.0, \"max_tokens\": 256, \"top_k\": 0, \"presence_penalty\": 0, \"frequency_penalty\": 0}\n",
      "header: You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 5.63e-05\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.00421875\n",
      "cost_per_100_completions: 0.0421875\n",
      "link: https://docs.aleph-alpha.com/docs/introduction/luminous/\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully\n",
      "[end of summary]\n",
      "\n",
      "Updates for model_id luminous-supreme-control appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id luminous-supreme-control**\n",
      "\n",
      "long_name: luminous-supreme-control\n",
      "short_name: luminous-supreme-control\n",
      "longcode: LUMS\n",
      "model_family: Aleph Alpha\n",
      "provider: Aleph\n",
      "provider_type: Commerical\n",
      "model_type: Instruct\n",
      "default_params: {\"temperature\": 0.0, \"top_p\": 0.0, \"max_tokens\": 64, \"top_k\": 0, \"presence_penalty\": 0, \"frequency_penalty\": 0}\n",
      "selected_params: {\"temperature\": 1.0, \"top_p\": 0.0, \"max_tokens\": 256, \"top_k\": 0, \"presence_penalty\": 0, \"frequency_penalty\": 0}\n",
      "header: You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 0.00021875\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.01640625\n",
      "cost_per_100_completions: 0.1640625\n",
      "link: https://docs.aleph-alpha.com/docs/introduction/luminous/\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully\n",
      "[end of summary]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_id_to_update in model_ids:\n",
    "    base_updates = {\n",
    "        \"header\": BASE_HEADER_INSTRUCT,\n",
    "        \"footer\": BASE_FOOTER,\n",
    "        \"default_params\": {\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_p\": 0.0,\n",
    "            \"max_tokens\": 64,\n",
    "            \"top_k\": 0,\n",
    "            \"presence_penalty\": 0,\n",
    "            \"frequency_penalty\": 0,\n",
    "        },\n",
    "        \"in_yaml\": \"Yes\",\n",
    "        \"params_tested\": \"Yes\",\n",
    "        \"status\": \"Deployed Successfully\",\n",
    "    }\n",
    "\n",
    "    selected_params_overrides = {\n",
    "        \"max_tokens\": MAX_TOKENS,  # Override the max_tokens\n",
    "        \"temperature\": 1.0,  # Override temp because it's deterministic\n",
    "    }\n",
    "\n",
    "    model_df = update_model_info(\n",
    "        PATH,\n",
    "        model_df,\n",
    "        provider,\n",
    "        model_id_to_update,\n",
    "        base_updates,\n",
    "        selected_params_overrides,\n",
    "    )\n",
    "\n",
    "    print_model_summary_from_df_row(model_df, model_id_to_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace\n",
    "\n",
    "Huggingface (HF) provides our access to open-source models via two paths: (1) the HF API with text_generation end points, and (2) custom inference endpoints.\n",
    "\n",
    "It provides a unique challenge because every model has slightly different setup.\n",
    "\n",
    "Note that without additional information use the following defaults:\n",
    "\n",
    "Parameters are taken from here: https://huggingface.co/docs/transformers/main_classes/text_generation.\n",
    "\n",
    "UPDATE: Note after testing (and getting lots of non-descript 422 errors), we found that top-p could not be at the strict upper bound.\n",
    "We got in contact with HF, and they suggested: \"All temperatures should always be set above 0.1, all top_p above 0 and below 1, and top_k > 1.\". \n",
    "We update top_p to 0.9. \n",
    "We also are updating min_tokens because we noted a few models returning empty strings when min_tokens was not strictly greater than 0.\n",
    "\n",
    "\n",
    "Base chat template: \n",
    "```\n",
    "\"Human: <prompt>\\nAssistant: <reply>\\nHuman: <prompt>\\nAssistant: \"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_defaults = {\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"top_k\": 50,\n",
    "    \"min_tokens\": 0,  # min_new_tokens\n",
    "    \"max_tokens\": 20,  # max_new_tokens\n",
    "    \"is_llama\": 0,\n",
    "    \"is_vicuna\": 0,\n",
    "    \"is_falcon\": 0,\n",
    "    \"is_pythia\": 0,\n",
    "    \"is_guanaco\": 0,\n",
    "    \"is_zephyr\": 0,\n",
    "}\n",
    "\n",
    "# Note max tokens here is max NEW tokens so diff to the other providers\n",
    "UPDATED_MAX_TOKENS_EXC_PROMPT = 200\n",
    "# Min tokens set to force response\n",
    "UPDATED_MIN_TOKENS = 10\n",
    "# Top-p set below strict upper limit\n",
    "UPDATED_TOP_P = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Model Name>:<Model ID> for provider HuggingFace-API:\n",
      "flan-t5-xxl:google/flan-t5-xxl, see: https://huggingface.co/google/flan-t5-xxl\n",
      "zephyr-7b-beta:HuggingFaceH4/zephyr-7b-beta, see: https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\n",
      "llama-2-13b-chat:meta-llama/Llama-2-13b-chat-hf, see: https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\n",
      "llama-2-70b-chat:meta-llama/Llama-2-70b-chat-hf, see: https://huggingface.co/meta-llama/Llama-2-70b-chat-hf\n",
      "llama-2-7b-chat:meta-llama/Llama-2-7b-chat-hf, see: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
      "mistral-7b-instruct:mistralai/Mistral-7B-Instruct-v0.1, see: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
      "pythia-12b:OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5, see: https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\n",
      "falcon-7b-instruct:tiiuae/falcon-7b-instruct, see: https://huggingface.co/tiiuae/falcon-7b-instruct\n",
      "guanaco-33b:timdettmers/guanaco-33b-merged, see: https://huggingface.co/timdettmers/guanaco-33b-merged\n"
     ]
    }
   ],
   "source": [
    "provider = \"HuggingFace-API\"\n",
    "model_ids = print_model_ids_by_provider(model_df, provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HF API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama-2-chat Models\n",
    "\n",
    "We have three variants of Llama-2-chat: 7b, 13b, 70b.\n",
    "\n",
    "They provide specific default params: https://github.com/facebookresearch/llama/blob/main/llama/generation.py. We find temp = 0.6 to be a bit low (only very minor word switches), increasing to 1.0.\n",
    "\n",
    "The special llama dialog format can also be found on the same link, and is also explained here: https://gpus.llm-utils.org/llama-2-prompt-template/.\n",
    "\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "{your_system_message}\n",
    "<</SYS>>\n",
    "\n",
    "{user_message_1} [/INST]\n",
    "```\n",
    "\n",
    "Also mentioned in https://github.com/huggingface/chat-ui/issues/382\n",
    "\n",
    "```\n",
    "{{#each history}}\n",
    "<s>[INST] {{#if @first}}<<SYS>>{{{@root.preprompt}}}<</SYS>>\n",
    "\n",
    "{{/if}}{{user}} [/INST]{{#unless @last}}{{assistant}} </s>{{/unless}}\n",
    "{{/each}}\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates for model_id meta-llama/Llama-2-7b-chat-hf appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id meta-llama/Llama-2-7b-chat-hf**\n",
      "\n",
      "long_name: meta-llama/Llama-2-7b-chat-hf\n",
      "short_name: llama-2-7b-chat\n",
      "longcode: LL7\n",
      "model_family: Meta\n",
      "provider: HuggingFace-API\n",
      "provider_type: Open Access\n",
      "model_type: Chat\n",
      "default_params: {\"temperature\": 0.6, \"top_p\": 0.9, \"top_k\": 50, \"min_tokens\": 0, \"max_tokens\": null, \"is_llama\": 1, \"is_vicuna\": 0, \"is_falcon\": 0, \"is_pythia\": 0, \"is_guanaco\": 0, \"is_zephyr\": 0}\n",
      "selected_params: {\"temperature\": 1.0, \"top_p\": 0.9, \"top_k\": 50, \"min_tokens\": 10, \"max_tokens\": 200, \"is_llama\": 1, \"is_vicuna\": 0, \"is_falcon\": 0, \"is_pythia\": 0, \"is_guanaco\": 0, \"is_zephyr\": 0}\n",
      "header: You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 0.0\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.0\n",
      "cost_per_100_completions: 0.0\n",
      "link: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully\n",
      "[end of summary]\n",
      "\n",
      "Updates for model_id meta-llama/Llama-2-13b-chat-hf appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id meta-llama/Llama-2-13b-chat-hf**\n",
      "\n",
      "long_name: meta-llama/Llama-2-13b-chat-hf\n",
      "short_name: llama-2-13b-chat\n",
      "longcode: LL13\n",
      "model_family: Meta\n",
      "provider: HuggingFace-API\n",
      "provider_type: Open Access\n",
      "model_type: Chat\n",
      "default_params: {\"temperature\": 0.6, \"top_p\": 0.9, \"top_k\": 50, \"min_tokens\": 0, \"max_tokens\": null, \"is_llama\": 1, \"is_vicuna\": 0, \"is_falcon\": 0, \"is_pythia\": 0, \"is_guanaco\": 0, \"is_zephyr\": 0}\n",
      "selected_params: {\"temperature\": 1.0, \"top_p\": 0.9, \"top_k\": 50, \"min_tokens\": 10, \"max_tokens\": 200, \"is_llama\": 1, \"is_vicuna\": 0, \"is_falcon\": 0, \"is_pythia\": 0, \"is_guanaco\": 0, \"is_zephyr\": 0}\n",
      "header: You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 0.0\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.0\n",
      "cost_per_100_completions: 0.0\n",
      "link: https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully (but fails intermittently)\n",
      "[end of summary]\n",
      "\n",
      "Updates for model_id meta-llama/Llama-2-70b-chat-hf appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id meta-llama/Llama-2-70b-chat-hf**\n",
      "\n",
      "long_name: meta-llama/Llama-2-70b-chat-hf\n",
      "short_name: llama-2-70b-chat\n",
      "longcode: LL70\n",
      "model_family: Meta\n",
      "provider: HuggingFace-API\n",
      "provider_type: Open Access\n",
      "model_type: Chat\n",
      "default_params: {\"temperature\": 0.6, \"top_p\": 0.9, \"top_k\": 50, \"min_tokens\": 0, \"max_tokens\": null, \"is_llama\": 1, \"is_vicuna\": 0, \"is_falcon\": 0, \"is_pythia\": 0, \"is_guanaco\": 0, \"is_zephyr\": 0}\n",
      "selected_params: {\"temperature\": 1.0, \"top_p\": 0.9, \"top_k\": 50, \"min_tokens\": 10, \"max_tokens\": 200, \"is_llama\": 1, \"is_vicuna\": 0, \"is_falcon\": 0, \"is_pythia\": 0, \"is_guanaco\": 0, \"is_zephyr\": 0}\n",
      "header: You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 0.0\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.0\n",
      "cost_per_100_completions: 0.0\n",
      "link: https://huggingface.co/meta-llama/Llama-2-70b-chat-hf\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully\n",
      "[end of summary]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_ids = [\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "]\n",
    "for model_id_to_update in model_ids:\n",
    "    if \"13b\" in model_id_to_update:\n",
    "        status = \"Deployed Successfully (but fails intermittently)\"\n",
    "    else:\n",
    "        status = \"Deployed Successfully\"\n",
    "    base_updates = {\n",
    "        \"header\": BASE_HEADER,\n",
    "        \"footer\": BASE_FOOTER,\n",
    "        \"default_params\": {\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,  # Using HF default here\n",
    "            \"min_tokens\": 0,  # min_new_tokens, using HF default here\n",
    "            \"max_tokens\": None,  # max_new_tokens\n",
    "            \"is_llama\": 1,\n",
    "            \"is_vicuna\": 0,\n",
    "            \"is_falcon\": 0,\n",
    "            \"is_pythia\": 0,\n",
    "            \"is_guanaco\": 0,\n",
    "            \"is_zephyr\": 0,\n",
    "        },\n",
    "        \"in_yaml\": \"Yes\",\n",
    "        \"params_tested\": \"Yes\",\n",
    "        \"status\": status,\n",
    "    }\n",
    "\n",
    "    selected_params_overrides = {\n",
    "        \"max_tokens\": UPDATED_MAX_TOKENS_EXC_PROMPT,\n",
    "        \"min_tokens\": UPDATED_MIN_TOKENS,  # Override the max_tokens\n",
    "        \"temperature\": 1.0,  # Override the max_tokens\n",
    "    }\n",
    "\n",
    "    model_df = update_model_info(\n",
    "        PATH,\n",
    "        model_df,\n",
    "        provider,\n",
    "        model_id_to_update,\n",
    "        base_updates,\n",
    "        selected_params_overrides,\n",
    "    )\n",
    "\n",
    "    print_model_summary_from_df_row(model_df, model_id_to_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Falcon Models\n",
    "\n",
    "No clear default parameters provided on model card, using `hf_defaults`.\n",
    "\n",
    "\n",
    "No explicit template given on model cards but this suggests there is a consistent format: https://github.com/huggingface/chat-ui/issues/382\n",
    "\n",
    "```\n",
    "{{system message}}\n",
    "\n",
    "User: {{user message 1}}\n",
    "Falcon: {{assistant message 1}}\n",
    "User: {{user message 2}}\n",
    "Falcon:\n",
    "```\n",
    "Note no space after Falcon:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates for model_id tiiuae/falcon-7b-instruct appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id tiiuae/falcon-7b-instruct**\n",
      "\n",
      "long_name: tiiuae/falcon-7b-instruct\n",
      "short_name: falcon-7b-instruct\n",
      "longcode: FAL7\n",
      "model_family: Other OA\n",
      "provider: HuggingFace-API\n",
      "provider_type: Open Access\n",
      "model_type: Instruct\n",
      "default_params: {\"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 50, \"min_tokens\": 0, \"max_tokens\": 20, \"is_llama\": 0, \"is_vicuna\": 0, \"is_falcon\": 0, \"is_pythia\": 0, \"is_guanaco\": 0, \"is_zephyr\": 0}\n",
      "selected_params: {\"temperature\": 1.0, \"top_p\": 0.9, \"top_k\": 50, \"min_tokens\": 10, \"max_tokens\": 200, \"is_llama\": 0, \"is_vicuna\": 0, \"is_falcon\": 1, \"is_pythia\": 0, \"is_guanaco\": 0, \"is_zephyr\": 0}\n",
      "header: You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 0.0\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.0\n",
      "cost_per_100_completions: 0.0\n",
      "link: https://huggingface.co/tiiuae/falcon-7b-instruct\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully\n",
      "[end of summary]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_ids = [\n",
    "    \"tiiuae/falcon-7b-instruct\",\n",
    "    # \"tiiuae/falcon-180B-chat\", (fails too often)\n",
    "]\n",
    "for model_id_to_update in model_ids:\n",
    "    base_updates = {\n",
    "        \"header\": BASE_HEADER_INSTRUCT,\n",
    "        \"footer\": BASE_FOOTER,\n",
    "        \"default_params\": hf_defaults,\n",
    "        \"in_yaml\": \"Yes\",\n",
    "        \"params_tested\": \"Yes\",\n",
    "        \"status\": \"Deployed Successfully\",\n",
    "    }\n",
    "\n",
    "    selected_params_overrides = {\n",
    "        \"max_tokens\": UPDATED_MAX_TOKENS_EXC_PROMPT,\n",
    "        \"min_tokens\": UPDATED_MIN_TOKENS,\n",
    "        \"top_p\": UPDATED_TOP_P,\n",
    "        \"is_falcon\": 1,\n",
    "    }\n",
    "\n",
    "    model_df = update_model_info(\n",
    "        PATH,\n",
    "        model_df,\n",
    "        provider,\n",
    "        model_id_to_update,\n",
    "        base_updates,\n",
    "        selected_params_overrides,\n",
    "    )\n",
    "\n",
    "    print_model_summary_from_df_row(model_df, model_id_to_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral-7B-Instruct\n",
    "No clear default parameters provided on model card, using `hf_defaults`. However, we add the is_llama flag for the prompt template to mimic the llama template.\n",
    "\n",
    "Mistral follows the chat template of llama: \"In order to leverage instruction fine-tuning, your prompt should be surrounded by [INST] and [/INST] tokens.\" This is documented on the model card: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
    "\n",
    "```\n",
    "text = \"<s>[INST] What is your favourite condiment? [/INST]\"\n",
    "\"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> \"\n",
    "\"[INST] Do you have mayonnaise recipes? [/INST]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates for model_id mistralai/Mistral-7B-Instruct-v0.1 appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id mistralai/Mistral-7B-Instruct-v0.1**\n",
      "\n",
      "long_name: mistralai/Mistral-7B-Instruct-v0.1\n",
      "short_name: mistral-7b-instruct\n",
      "longcode: MIST\n",
      "model_family: Mistral\n",
      "provider: HuggingFace-API\n",
      "provider_type: Open Access\n",
      "model_type: Instruct\n",
      "default_params: {\"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 50, \"min_tokens\": 0, \"max_tokens\": 20, \"is_llama\": 0, \"is_vicuna\": 0, \"is_falcon\": 0, \"is_pythia\": 0, \"is_guanaco\": 0, \"is_zephyr\": 0}\n",
      "selected_params: {\"temperature\": 1.0, \"top_p\": 0.9, \"top_k\": 50, \"min_tokens\": 10, \"max_tokens\": 200, \"is_llama\": 1, \"is_vicuna\": 0, \"is_falcon\": 0, \"is_pythia\": 0, \"is_guanaco\": 0, \"is_zephyr\": 0}\n",
      "header: You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 0.0\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.0\n",
      "cost_per_100_completions: 0.0\n",
      "link: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully\n",
      "[end of summary]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id_to_update = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "base_updates = {\n",
    "    \"header\": BASE_HEADER_INSTRUCT,\n",
    "    \"footer\": BASE_FOOTER,\n",
    "    \"default_params\": hf_defaults,\n",
    "    \"in_yaml\": \"Yes\",\n",
    "    \"params_tested\": \"Yes\",\n",
    "    \"status\": \"Deployed Successfully\",\n",
    "}\n",
    "\n",
    "selected_params_overrides = {\n",
    "    \"max_tokens\": UPDATED_MAX_TOKENS_EXC_PROMPT,\n",
    "    \"min_tokens\": UPDATED_MIN_TOKENS,\n",
    "    \"top_p\": UPDATED_TOP_P,\n",
    "    \"is_llama\": 1,\n",
    "}\n",
    "\n",
    "model_df = update_model_info(\n",
    "    PATH,\n",
    "    model_df,\n",
    "    provider,\n",
    "    model_id_to_update,\n",
    "    base_updates,\n",
    "    selected_params_overrides,\n",
    ")\n",
    "\n",
    "print_model_summary_from_df_row(model_df, model_id_to_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pythia-12b\n",
    "\n",
    "No clear default parameters provided on model card, using `hf_defaults`.\n",
    "\n",
    "Chat format can be found on model card: https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5. Two special tokens are used to mark the beginning of user and assistant turns: <|prompter|> and <|assistant|>. Each turn ends with a <|endoftext|> token.\n",
    "\n",
    "```\n",
    "<|prompter|>What is a meme, and what's the history behind this word?<|endoftext|><|assistant|>\n",
    "```\n",
    "\n",
    "Also discussed here: https://github.com/huggingface/chat-ui/issues/382\n",
    "```\n",
    "{{{preprompt}}}\n",
    "{{#each history}}\n",
    "<|prompter|>{{user}}<|endoftext|><|assistant|>{{#unless @last}}{{assistant}}<|endoftext|>{{/unless}}\n",
    "{{/each}}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates for model_id OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5 appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5**\n",
      "\n",
      "long_name: OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\n",
      "short_name: pythia-12b\n",
      "longcode: PYTH\n",
      "model_family: Other OA\n",
      "provider: HuggingFace-API\n",
      "provider_type: Open Access\n",
      "model_type: Chat\n",
      "default_params: {\"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 50, \"min_tokens\": 0, \"max_tokens\": 20, \"is_llama\": 0, \"is_vicuna\": 0, \"is_falcon\": 0, \"is_pythia\": 0, \"is_guanaco\": 0, \"is_zephyr\": 0}\n",
      "selected_params: {\"temperature\": 1.0, \"top_p\": 0.9, \"top_k\": 50, \"min_tokens\": 10, \"max_tokens\": 200, \"is_llama\": 0, \"is_vicuna\": 0, \"is_falcon\": 0, \"is_pythia\": 1, \"is_guanaco\": 0, \"is_zephyr\": 0}\n",
      "header: You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 0.0\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.0\n",
      "cost_per_100_completions: 0.0\n",
      "link: https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully\n",
      "[end of summary]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id_to_update = \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\n",
    "base_updates = {\n",
    "    \"header\": BASE_HEADER_INSTRUCT,\n",
    "    \"footer\": BASE_FOOTER,\n",
    "    \"default_params\": hf_defaults,\n",
    "    \"in_yaml\": \"Yes\",\n",
    "    \"params_tested\": \"Yes\",\n",
    "    \"status\": \"Deployed Successfully\",\n",
    "}\n",
    "\n",
    "selected_params_overrides = {\n",
    "    \"max_tokens\": UPDATED_MAX_TOKENS_EXC_PROMPT,\n",
    "    \"min_tokens\": UPDATED_MIN_TOKENS,\n",
    "    \"top_p\": UPDATED_TOP_P,\n",
    "    \"is_pythia\": 1,\n",
    "}\n",
    "\n",
    "model_df = update_model_info(\n",
    "    PATH,\n",
    "    model_df,\n",
    "    provider,\n",
    "    model_id_to_update,\n",
    "    base_updates,\n",
    "    selected_params_overrides,\n",
    ")\n",
    "\n",
    "print_model_summary_from_df_row(model_df, model_id_to_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guanaco-33b\n",
    "\n",
    "No clear default parameters provided on model card, using `hf_defaults`.\n",
    "\n",
    "\n",
    "Chat format is suggested here: https://huggingface.co/timdettmers/guanaco-33b-merged/discussions/4. Which is very similar to our base format.\n",
    "\n",
    "```\n",
    "{system string}\n",
    "### Human: {input}\n",
    "### Assistant: {output}\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates for model_id timdettmers/guanaco-33b-merged appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id timdettmers/guanaco-33b-merged**\n",
      "\n",
      "long_name: timdettmers/guanaco-33b-merged\n",
      "short_name: guanaco-33b\n",
      "longcode: GUAN\n",
      "model_family: Other OA\n",
      "provider: HuggingFace-API\n",
      "provider_type: Open Access\n",
      "model_type: Instruct\n",
      "default_params: {\"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 50, \"min_tokens\": 0, \"max_tokens\": 20, \"is_llama\": 0, \"is_vicuna\": 0, \"is_falcon\": 0, \"is_pythia\": 0, \"is_guanaco\": 0, \"is_zephyr\": 0}\n",
      "selected_params: {\"temperature\": 1.0, \"top_p\": 0.9, \"top_k\": 50, \"min_tokens\": 10, \"max_tokens\": 200, \"is_llama\": 0, \"is_vicuna\": 0, \"is_falcon\": 0, \"is_pythia\": 0, \"is_guanaco\": 1, \"is_zephyr\": 0}\n",
      "header: You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 0.0\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.0\n",
      "cost_per_100_completions: 0.0\n",
      "link: https://huggingface.co/timdettmers/guanaco-33b-merged\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully\n",
      "[end of summary]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id_to_update = \"timdettmers/guanaco-33b-merged\"\n",
    "base_updates = {\n",
    "    \"header\": BASE_HEADER_INSTRUCT,\n",
    "    \"footer\": BASE_FOOTER,\n",
    "    \"default_params\": hf_defaults,\n",
    "    \"in_yaml\": \"Yes\",\n",
    "    \"params_tested\": \"Yes\",\n",
    "    \"status\": \"Deployed Successfully\",\n",
    "}\n",
    "\n",
    "selected_params_overrides = {\n",
    "    \"max_tokens\": UPDATED_MAX_TOKENS_EXC_PROMPT,\n",
    "    \"min_tokens\": UPDATED_MIN_TOKENS,\n",
    "    \"top_p\": UPDATED_TOP_P,\n",
    "    \"is_guanaco\": 1,\n",
    "}\n",
    "\n",
    "model_df = update_model_info(\n",
    "    PATH,\n",
    "    model_df,\n",
    "    provider,\n",
    "    model_id_to_update,\n",
    "    base_updates,\n",
    "    selected_params_overrides,\n",
    ")\n",
    "\n",
    "print_model_summary_from_df_row(model_df, model_id_to_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zephyr-7b\n",
    "\n",
    "No clear default parameters provided on model card, using `hf_defaults`.\n",
    "\n",
    "Zephyr uses tokenizer's chat template to format each message:\n",
    "```\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "```\n",
    "\n",
    "Underlying:\n",
    "```\n",
    "<|system|>\n",
    "You are a friendly chatbot who always responds in the style of a pirate.</s>\n",
    "<|user|>\n",
    "How many helicopters can a human eat in one sitting?</s>\n",
    "<|assistant|>\n",
    "Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\n",
    "```\n",
    "\n",
    "This looks a bit similar to OpenAssistant Pythia-12b but with different end of turn tags and different \"role\" tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates for model_id HuggingFaceH4/zephyr-7b-beta appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id HuggingFaceH4/zephyr-7b-beta**\n",
      "\n",
      "long_name: HuggingFaceH4/zephyr-7b-beta\n",
      "short_name: zephyr-7b-beta\n",
      "longcode: ZEPH\n",
      "model_family: HuggingFace\n",
      "provider: HuggingFace-API\n",
      "provider_type: Open Access\n",
      "model_type: Chat\n",
      "default_params: {\"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 50, \"min_tokens\": 0, \"max_tokens\": 20, \"is_llama\": 0, \"is_vicuna\": 0, \"is_falcon\": 0, \"is_pythia\": 0, \"is_guanaco\": 0, \"is_zephyr\": 0}\n",
      "selected_params: {\"temperature\": 1.0, \"top_p\": 0.9, \"top_k\": 50, \"min_tokens\": 10, \"max_tokens\": 200, \"is_llama\": 0, \"is_vicuna\": 0, \"is_falcon\": 0, \"is_pythia\": 0, \"is_guanaco\": 0, \"is_zephyr\": 1}\n",
      "header: You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 0.0\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.0\n",
      "cost_per_100_completions: 0.0\n",
      "link: https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed\n",
      "[end of summary]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id_to_update = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "base_updates = {\n",
    "    \"header\": BASE_HEADER_INSTRUCT,\n",
    "    \"footer\": BASE_FOOTER,\n",
    "    \"default_params\": hf_defaults,\n",
    "    \"in_yaml\": \"Yes\",\n",
    "    \"params_tested\": \"Yes\",\n",
    "    \"status\": \"Deployed\",\n",
    "}\n",
    "\n",
    "selected_params_overrides = {\n",
    "    \"max_tokens\": UPDATED_MAX_TOKENS_EXC_PROMPT,\n",
    "    \"min_tokens\": UPDATED_MIN_TOKENS,\n",
    "    \"top_p\": UPDATED_TOP_P,\n",
    "    \"is_zephyr\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "model_df = update_model_info(\n",
    "    PATH,\n",
    "    model_df,\n",
    "    provider,\n",
    "    model_id_to_update,\n",
    "    base_updates,\n",
    "    selected_params_overrides,\n",
    ")\n",
    "\n",
    "print_model_summary_from_df_row(model_df, model_id_to_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flan-T5-XXL\n",
    "\n",
    "No clear default parameters provided on model card, using `hf_defaults`.\n",
    "\n",
    "No clear template given, using base chat template:\n",
    "```\n",
    "\"Human: <prompt>\\nAssistant: <reply>\\nHuman:<prompt>\\nAssistant: \"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates for model_id google/flan-t5-xxl appended to the log file.\n",
      "\n",
      "\n",
      "**Model summary for model_id google/flan-t5-xxl**\n",
      "\n",
      "long_name: google/flan-t5-xxl\n",
      "short_name: flan-t5-xxl\n",
      "longcode: FLAN\n",
      "model_family: Google\n",
      "provider: HuggingFace-API\n",
      "provider_type: Open Access\n",
      "model_type: Instruct\n",
      "default_params: {\"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 50, \"min_tokens\": 0, \"max_tokens\": 20, \"is_llama\": 0, \"is_vicuna\": 0, \"is_falcon\": 0, \"is_pythia\": 0, \"is_guanaco\": 0, \"is_zephyr\": 0}\n",
      "selected_params: {\"temperature\": 1.0, \"top_p\": 0.9, \"top_k\": 50, \"min_tokens\": 10, \"max_tokens\": 200, \"is_llama\": 0, \"is_vicuna\": 0, \"is_falcon\": 0, \"is_pythia\": 0, \"is_guanaco\": 0, \"is_zephyr\": 0}\n",
      "header: You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit.\n",
      "footer: \n",
      "cost_per_token: 0.0\n",
      "tokens_per_completion: 75\n",
      "cost_per_completion: 0.0\n",
      "cost_per_100_completions: 0.0\n",
      "link: https://huggingface.co/google/flan-t5-xxl\n",
      "in_yaml: Yes\n",
      "params_tested: Yes\n",
      "endpoint_live: Yes\n",
      "status: Deployed Successfully\n",
      "[end of summary]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id_to_update = \"google/flan-t5-xxl\"\n",
    "base_updates = {\n",
    "    \"header\": BASE_HEADER_INSTRUCT,\n",
    "    \"footer\": BASE_FOOTER,\n",
    "    \"default_params\": hf_defaults,\n",
    "    \"in_yaml\": \"Yes\",\n",
    "    \"params_tested\": \"Yes\",\n",
    "    \"status\": \"Deployed Successfully\",\n",
    "}\n",
    "\n",
    "selected_params_overrides = {\n",
    "    \"max_tokens\": UPDATED_MAX_TOKENS_EXC_PROMPT,\n",
    "    \"min_tokens\": UPDATED_MIN_TOKENS,\n",
    "    \"top_p\": UPDATED_TOP_P,\n",
    "}\n",
    "\n",
    "\n",
    "model_df = update_model_info(\n",
    "    PATH,\n",
    "    model_df,\n",
    "    provider,\n",
    "    model_id_to_update,\n",
    "    base_updates,\n",
    "    selected_params_overrides,\n",
    ")\n",
    "\n",
    "print_model_summary_from_df_row(model_df, model_id_to_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-export Model Data as JSONL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_name</th>\n",
       "      <th>short_name</th>\n",
       "      <th>longcode</th>\n",
       "      <th>model_family</th>\n",
       "      <th>provider</th>\n",
       "      <th>provider_type</th>\n",
       "      <th>model_type</th>\n",
       "      <th>default_params</th>\n",
       "      <th>selected_params</th>\n",
       "      <th>header</th>\n",
       "      <th>footer</th>\n",
       "      <th>cost_per_token</th>\n",
       "      <th>tokens_per_completion</th>\n",
       "      <th>cost_per_completion</th>\n",
       "      <th>cost_per_100_completions</th>\n",
       "      <th>link</th>\n",
       "      <th>in_yaml</th>\n",
       "      <th>params_tested</th>\n",
       "      <th>endpoint_live</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claude-2</td>\n",
       "      <td>claude-2</td>\n",
       "      <td>CL2</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>Commerical</td>\n",
       "      <td>Chat</td>\n",
       "      <td>{\"temperature\": 1.0, \"top_p\": 0.7, \"presence_p...</td>\n",
       "      <td>{\"temperature\": 1.0, \"top_p\": 0.7, \"presence_p...</td>\n",
       "      <td>You are a conversational assistant. Limit your...</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://docs.anthropic.com/claude/docs/legacy-...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>claude-2.1</td>\n",
       "      <td>claude-2.1</td>\n",
       "      <td>CL2*</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>Commerical</td>\n",
       "      <td>Chat</td>\n",
       "      <td>{\"temperature\": 1.0, \"top_p\": 0.7, \"presence_p...</td>\n",
       "      <td>{\"temperature\": 1.0, \"top_p\": 0.7, \"presence_p...</td>\n",
       "      <td>You are a conversational assistant. Limit your...</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://docs.anthropic.com/claude/docs/legacy-...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>claude-instant-1</td>\n",
       "      <td>claude-instant-1</td>\n",
       "      <td>CL1</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>Commerical</td>\n",
       "      <td>Chat</td>\n",
       "      <td>{\"temperature\": 1.0, \"top_p\": 0.7, \"presence_p...</td>\n",
       "      <td>{\"temperature\": 1.0, \"top_p\": 0.7, \"presence_p...</td>\n",
       "      <td>You are a conversational assistant. Limit your...</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://docs.anthropic.com/claude/docs/legacy-...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          long_name        short_name longcode model_family   provider  \\\n",
       "0          claude-2          claude-2      CL2    Anthropic  Anthropic   \n",
       "1        claude-2.1        claude-2.1     CL2*    Anthropic  Anthropic   \n",
       "2  claude-instant-1  claude-instant-1      CL1    Anthropic  Anthropic   \n",
       "\n",
       "  provider_type model_type                                     default_params  \\\n",
       "0    Commerical       Chat  {\"temperature\": 1.0, \"top_p\": 0.7, \"presence_p...   \n",
       "1    Commerical       Chat  {\"temperature\": 1.0, \"top_p\": 0.7, \"presence_p...   \n",
       "2    Commerical       Chat  {\"temperature\": 1.0, \"top_p\": 0.7, \"presence_p...   \n",
       "\n",
       "                                     selected_params  \\\n",
       "0  {\"temperature\": 1.0, \"top_p\": 0.7, \"presence_p...   \n",
       "1  {\"temperature\": 1.0, \"top_p\": 0.7, \"presence_p...   \n",
       "2  {\"temperature\": 1.0, \"top_p\": 0.7, \"presence_p...   \n",
       "\n",
       "                                              header footer  cost_per_token  \\\n",
       "0  You are a conversational assistant. Limit your...                    0.0   \n",
       "1  You are a conversational assistant. Limit your...                    0.0   \n",
       "2  You are a conversational assistant. Limit your...                    0.0   \n",
       "\n",
       "   tokens_per_completion  cost_per_completion  cost_per_100_completions  \\\n",
       "0                     75                  0.0                       0.0   \n",
       "1                     75                  0.0                       0.0   \n",
       "2                     75                  0.0                       0.0   \n",
       "\n",
       "                                                link in_yaml params_tested  \\\n",
       "0  https://docs.anthropic.com/claude/docs/legacy-...     Yes           Yes   \n",
       "1  https://docs.anthropic.com/claude/docs/legacy-...     Yes           Yes   \n",
       "2  https://docs.anthropic.com/claude/docs/legacy-...     Yes           Yes   \n",
       "\n",
       "  endpoint_live                 status  \n",
       "0           Yes  Deployed Successfully  \n",
       "1           Yes  Deployed Successfully  \n",
       "2           Yes  Deployed Successfully  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"temperature\": 1.0, \"top_p\": 0.9, \"top_k\": 50, \"min_tokens\": 10, \"max_tokens\": 200, \"is_llama\": 0, \"is_vicuna\": 0, \"is_falcon\": 0, \"is_pythia\": 0, \"is_guanaco\": 1, \"is_zephyr\": 0}'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df[\"selected_params\"].iloc[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data for export\n",
    "drop_columns = [\n",
    "    \"footer\",\n",
    "    \"cost_per_token\",\n",
    "    \"tokens_per_completion\",\n",
    "    \"cost_per_completion\",\n",
    "    \"cost_per_100_completions\",\n",
    "]\n",
    "model_df.drop(columns=drop_columns, inplace=True)\n",
    "model_df = model_df.rename(\n",
    "    columns={\n",
    "        \"provider\": \"model_provider\",\n",
    "        \"provider_type\": \"model_provider_type\",\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Function to format the dictionary\n",
    "def format_dict(d):\n",
    "    for key in d:\n",
    "        if key in float_keys:\n",
    "            d[key] = (\n",
    "                f\"{d[key]:.1f}\"  # fmt 1 decimal point to avoid precision change on reload\n",
    "            )\n",
    "        elif key in int_keys:\n",
    "            try:\n",
    "                d[key] = (\n",
    "                    f\"{d[key]:.0f}\"  # fmt as str int to avoid precision change on reload\n",
    "                )\n",
    "            except:\n",
    "                d[key] = f\"{d[key]}\"\n",
    "    return d\n",
    "\n",
    "\n",
    "dictionary_columns = [\"default_params\", \"selected_params\"]\n",
    "# Specify the formatting\n",
    "float_keys = [\"temperature\", \"top_p\", \"presence_penalty\", \"frequency_penalty\"]\n",
    "int_keys = [\"max_tokens\", \"min_tokens\", \"top_k\"] + [\n",
    "    c for c in model_df.columns if \"is_\" in c\n",
    "]\n",
    "for column in dictionary_columns:\n",
    "    model_df[column] = model_df[column].apply(lambda x: format_dict(json.loads(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_name</th>\n",
       "      <th>short_name</th>\n",
       "      <th>longcode</th>\n",
       "      <th>model_family</th>\n",
       "      <th>model_provider</th>\n",
       "      <th>model_provider_type</th>\n",
       "      <th>model_type</th>\n",
       "      <th>default_params</th>\n",
       "      <th>selected_params</th>\n",
       "      <th>header</th>\n",
       "      <th>link</th>\n",
       "      <th>in_yaml</th>\n",
       "      <th>params_tested</th>\n",
       "      <th>endpoint_live</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claude-2</td>\n",
       "      <td>claude-2</td>\n",
       "      <td>CL2</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>Commerical</td>\n",
       "      <td>Chat</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '0.7', 'presen...</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '0.7', 'presen...</td>\n",
       "      <td>You are a conversational assistant. Limit your...</td>\n",
       "      <td>https://docs.anthropic.com/claude/docs/legacy-...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>claude-2.1</td>\n",
       "      <td>claude-2.1</td>\n",
       "      <td>CL2*</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>Commerical</td>\n",
       "      <td>Chat</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '0.7', 'presen...</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '0.7', 'presen...</td>\n",
       "      <td>You are a conversational assistant. Limit your...</td>\n",
       "      <td>https://docs.anthropic.com/claude/docs/legacy-...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>claude-instant-1</td>\n",
       "      <td>claude-instant-1</td>\n",
       "      <td>CL1</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>Commerical</td>\n",
       "      <td>Chat</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '0.7', 'presen...</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '0.7', 'presen...</td>\n",
       "      <td>You are a conversational assistant. Limit your...</td>\n",
       "      <td>https://docs.anthropic.com/claude/docs/legacy-...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>command</td>\n",
       "      <td>command</td>\n",
       "      <td>COM</td>\n",
       "      <td>Cohere</td>\n",
       "      <td>Cohere</td>\n",
       "      <td>Commerical</td>\n",
       "      <td>Instruct</td>\n",
       "      <td>{'temperature': '0.3'}</td>\n",
       "      <td>{'temperature': '1.0', 'max_tokens': '256', 't...</td>\n",
       "      <td>You are a conversational assistant. Limit your...</td>\n",
       "      <td>https://docs.cohere.com/docs/models</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>command-light</td>\n",
       "      <td>command-light</td>\n",
       "      <td>COML</td>\n",
       "      <td>Cohere</td>\n",
       "      <td>Cohere</td>\n",
       "      <td>Commerical</td>\n",
       "      <td>Instruct</td>\n",
       "      <td>{'temperature': '0.3'}</td>\n",
       "      <td>{'temperature': '1.0', 'max_tokens': '256', 't...</td>\n",
       "      <td>You are a conversational assistant. Limit your...</td>\n",
       "      <td>https://docs.cohere.com/docs/models</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>command-nightly</td>\n",
       "      <td>command-nightly</td>\n",
       "      <td>COMN</td>\n",
       "      <td>Cohere</td>\n",
       "      <td>Cohere</td>\n",
       "      <td>Commerical</td>\n",
       "      <td>Instruct</td>\n",
       "      <td>{'temperature': '0.3'}</td>\n",
       "      <td>{'temperature': '1.0', 'max_tokens': '256', 't...</td>\n",
       "      <td>You are a conversational assistant. Limit your...</td>\n",
       "      <td>https://docs.cohere.com/docs/models</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>google/flan-t5-xxl</td>\n",
       "      <td>flan-t5-xxl</td>\n",
       "      <td>FLAN</td>\n",
       "      <td>Google</td>\n",
       "      <td>HuggingFace-API</td>\n",
       "      <td>Open Access</td>\n",
       "      <td>Instruct</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '1.0', 'top_k'...</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '0.9', 'top_k'...</td>\n",
       "      <td>You are a conversational assistant. The conver...</td>\n",
       "      <td>https://huggingface.co/google/flan-t5-xxl</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>GP3*</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>Commerical</td>\n",
       "      <td>Chat</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '1.0', 'presen...</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '1.0', 'presen...</td>\n",
       "      <td>You are a conversational assistant. Limit your...</td>\n",
       "      <td>https://platform.openai.com/docs/models/gpt-3-5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt-4</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>GPT4</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>Commerical</td>\n",
       "      <td>Chat</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '1.0', 'presen...</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '1.0', 'presen...</td>\n",
       "      <td>You are a conversational assistant. Limit your...</td>\n",
       "      <td>https://platform.openai.com/docs/models/gpt-4-...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-turbo</td>\n",
       "      <td>GPT4*</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>Commerical</td>\n",
       "      <td>Chat</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '1.0', 'presen...</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '1.0', 'presen...</td>\n",
       "      <td>You are a conversational assistant. Limit your...</td>\n",
       "      <td>https://platform.openai.com/docs/models/gpt-4-...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HuggingFaceH4/zephyr-7b-beta</td>\n",
       "      <td>zephyr-7b-beta</td>\n",
       "      <td>ZEPH</td>\n",
       "      <td>HuggingFace</td>\n",
       "      <td>HuggingFace-API</td>\n",
       "      <td>Open Access</td>\n",
       "      <td>Chat</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '1.0', 'top_k'...</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '0.9', 'top_k'...</td>\n",
       "      <td>You are a conversational assistant. The conver...</td>\n",
       "      <td>https://huggingface.co/HuggingFaceH4/zephyr-7b...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>luminous-extended-control</td>\n",
       "      <td>luminous-extended-control</td>\n",
       "      <td>LUMX</td>\n",
       "      <td>Aleph Alpha</td>\n",
       "      <td>Aleph</td>\n",
       "      <td>Commerical</td>\n",
       "      <td>Instruct</td>\n",
       "      <td>{'temperature': '0.0', 'top_p': '0.0', 'max_to...</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '0.0', 'max_to...</td>\n",
       "      <td>You are a conversational assistant. The conver...</td>\n",
       "      <td>https://docs.aleph-alpha.com/docs/introduction...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>luminous-supreme-control</td>\n",
       "      <td>luminous-supreme-control</td>\n",
       "      <td>LUMS</td>\n",
       "      <td>Aleph Alpha</td>\n",
       "      <td>Aleph</td>\n",
       "      <td>Commerical</td>\n",
       "      <td>Instruct</td>\n",
       "      <td>{'temperature': '0.0', 'top_p': '0.0', 'max_to...</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '0.0', 'max_to...</td>\n",
       "      <td>You are a conversational assistant. The conver...</td>\n",
       "      <td>https://docs.aleph-alpha.com/docs/introduction...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>meta-llama/Llama-2-13b-chat-hf</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>LL13</td>\n",
       "      <td>Meta</td>\n",
       "      <td>HuggingFace-API</td>\n",
       "      <td>Open Access</td>\n",
       "      <td>Chat</td>\n",
       "      <td>{'temperature': '0.6', 'top_p': '0.9', 'top_k'...</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '0.9', 'top_k'...</td>\n",
       "      <td>You are a conversational assistant. Limit your...</td>\n",
       "      <td>https://huggingface.co/meta-llama/Llama-2-13b-...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully (but fails intermittently)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>meta-llama/Llama-2-70b-chat-hf</td>\n",
       "      <td>llama-2-70b-chat</td>\n",
       "      <td>LL70</td>\n",
       "      <td>Meta</td>\n",
       "      <td>HuggingFace-API</td>\n",
       "      <td>Open Access</td>\n",
       "      <td>Chat</td>\n",
       "      <td>{'temperature': '0.6', 'top_p': '0.9', 'top_k'...</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '0.9', 'top_k'...</td>\n",
       "      <td>You are a conversational assistant. Limit your...</td>\n",
       "      <td>https://huggingface.co/meta-llama/Llama-2-70b-...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>meta-llama/Llama-2-7b-chat-hf</td>\n",
       "      <td>llama-2-7b-chat</td>\n",
       "      <td>LL7</td>\n",
       "      <td>Meta</td>\n",
       "      <td>HuggingFace-API</td>\n",
       "      <td>Open Access</td>\n",
       "      <td>Chat</td>\n",
       "      <td>{'temperature': '0.6', 'top_p': '0.9', 'top_k'...</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '0.9', 'top_k'...</td>\n",
       "      <td>You are a conversational assistant. Limit your...</td>\n",
       "      <td>https://huggingface.co/meta-llama/Llama-2-7b-c...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.1</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>MIST</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>HuggingFace-API</td>\n",
       "      <td>Open Access</td>\n",
       "      <td>Instruct</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '1.0', 'top_k'...</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '0.9', 'top_k'...</td>\n",
       "      <td>You are a conversational assistant. The conver...</td>\n",
       "      <td>https://huggingface.co/mistralai/Mistral-7B-In...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>models/chat-bison-001</td>\n",
       "      <td>palm-2</td>\n",
       "      <td>PALM</td>\n",
       "      <td>Google</td>\n",
       "      <td>Google</td>\n",
       "      <td>Commerical</td>\n",
       "      <td>Chat</td>\n",
       "      <td>{'temperature': '0.0', 'top_p': '0.9', 'max_to...</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '0.9', 'max_to...</td>\n",
       "      <td>You are a conversational assistant. Limit your...</td>\n",
       "      <td>https://ai.google.dev/palm_docs</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5</td>\n",
       "      <td>pythia-12b</td>\n",
       "      <td>PYTH</td>\n",
       "      <td>Other OA</td>\n",
       "      <td>HuggingFace-API</td>\n",
       "      <td>Open Access</td>\n",
       "      <td>Chat</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '1.0', 'top_k'...</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '0.9', 'top_k'...</td>\n",
       "      <td>You are a conversational assistant. The conver...</td>\n",
       "      <td>https://huggingface.co/OpenAssistant/oasst-sft...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tiiuae/falcon-7b-instruct</td>\n",
       "      <td>falcon-7b-instruct</td>\n",
       "      <td>FAL7</td>\n",
       "      <td>Other OA</td>\n",
       "      <td>HuggingFace-API</td>\n",
       "      <td>Open Access</td>\n",
       "      <td>Instruct</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '1.0', 'top_k'...</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '0.9', 'top_k'...</td>\n",
       "      <td>You are a conversational assistant. The conver...</td>\n",
       "      <td>https://huggingface.co/tiiuae/falcon-7b-instruct</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>timdettmers/guanaco-33b-merged</td>\n",
       "      <td>guanaco-33b</td>\n",
       "      <td>GUAN</td>\n",
       "      <td>Other OA</td>\n",
       "      <td>HuggingFace-API</td>\n",
       "      <td>Open Access</td>\n",
       "      <td>Instruct</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '1.0', 'top_k'...</td>\n",
       "      <td>{'temperature': '1.0', 'top_p': '0.9', 'top_k'...</td>\n",
       "      <td>You are a conversational assistant. The conver...</td>\n",
       "      <td>https://huggingface.co/timdettmers/guanaco-33b...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Deployed Successfully</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         long_name                 short_name  \\\n",
       "0                                         claude-2                   claude-2   \n",
       "1                                       claude-2.1                 claude-2.1   \n",
       "2                                 claude-instant-1           claude-instant-1   \n",
       "3                                          command                    command   \n",
       "4                                    command-light              command-light   \n",
       "5                                  command-nightly            command-nightly   \n",
       "6                               google/flan-t5-xxl                flan-t5-xxl   \n",
       "7                                    gpt-3.5-turbo              gpt-3.5-turbo   \n",
       "8                                            gpt-4                      gpt-4   \n",
       "9                               gpt-4-1106-preview                gpt-4-turbo   \n",
       "10                    HuggingFaceH4/zephyr-7b-beta             zephyr-7b-beta   \n",
       "11                       luminous-extended-control  luminous-extended-control   \n",
       "12                        luminous-supreme-control   luminous-supreme-control   \n",
       "13                  meta-llama/Llama-2-13b-chat-hf           llama-2-13b-chat   \n",
       "14                  meta-llama/Llama-2-70b-chat-hf           llama-2-70b-chat   \n",
       "15                   meta-llama/Llama-2-7b-chat-hf            llama-2-7b-chat   \n",
       "16              mistralai/Mistral-7B-Instruct-v0.1        mistral-7b-instruct   \n",
       "17                           models/chat-bison-001                     palm-2   \n",
       "18  OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5                 pythia-12b   \n",
       "19                       tiiuae/falcon-7b-instruct         falcon-7b-instruct   \n",
       "20                  timdettmers/guanaco-33b-merged                guanaco-33b   \n",
       "\n",
       "   longcode model_family   model_provider model_provider_type model_type  \\\n",
       "0       CL2    Anthropic        Anthropic          Commerical       Chat   \n",
       "1      CL2*    Anthropic        Anthropic          Commerical       Chat   \n",
       "2       CL1    Anthropic        Anthropic          Commerical       Chat   \n",
       "3       COM       Cohere           Cohere          Commerical   Instruct   \n",
       "4      COML       Cohere           Cohere          Commerical   Instruct   \n",
       "5      COMN       Cohere           Cohere          Commerical   Instruct   \n",
       "6      FLAN       Google  HuggingFace-API         Open Access   Instruct   \n",
       "7      GP3*       OpenAI           OpenAI          Commerical       Chat   \n",
       "8      GPT4       OpenAI           OpenAI          Commerical       Chat   \n",
       "9     GPT4*       OpenAI           OpenAI          Commerical       Chat   \n",
       "10     ZEPH  HuggingFace  HuggingFace-API         Open Access       Chat   \n",
       "11     LUMX  Aleph Alpha            Aleph          Commerical   Instruct   \n",
       "12     LUMS  Aleph Alpha            Aleph          Commerical   Instruct   \n",
       "13     LL13         Meta  HuggingFace-API         Open Access       Chat   \n",
       "14     LL70         Meta  HuggingFace-API         Open Access       Chat   \n",
       "15      LL7         Meta  HuggingFace-API         Open Access       Chat   \n",
       "16     MIST      Mistral  HuggingFace-API         Open Access   Instruct   \n",
       "17     PALM       Google           Google          Commerical       Chat   \n",
       "18     PYTH     Other OA  HuggingFace-API         Open Access       Chat   \n",
       "19     FAL7     Other OA  HuggingFace-API         Open Access   Instruct   \n",
       "20     GUAN     Other OA  HuggingFace-API         Open Access   Instruct   \n",
       "\n",
       "                                       default_params  \\\n",
       "0   {'temperature': '1.0', 'top_p': '0.7', 'presen...   \n",
       "1   {'temperature': '1.0', 'top_p': '0.7', 'presen...   \n",
       "2   {'temperature': '1.0', 'top_p': '0.7', 'presen...   \n",
       "3                              {'temperature': '0.3'}   \n",
       "4                              {'temperature': '0.3'}   \n",
       "5                              {'temperature': '0.3'}   \n",
       "6   {'temperature': '1.0', 'top_p': '1.0', 'top_k'...   \n",
       "7   {'temperature': '1.0', 'top_p': '1.0', 'presen...   \n",
       "8   {'temperature': '1.0', 'top_p': '1.0', 'presen...   \n",
       "9   {'temperature': '1.0', 'top_p': '1.0', 'presen...   \n",
       "10  {'temperature': '1.0', 'top_p': '1.0', 'top_k'...   \n",
       "11  {'temperature': '0.0', 'top_p': '0.0', 'max_to...   \n",
       "12  {'temperature': '0.0', 'top_p': '0.0', 'max_to...   \n",
       "13  {'temperature': '0.6', 'top_p': '0.9', 'top_k'...   \n",
       "14  {'temperature': '0.6', 'top_p': '0.9', 'top_k'...   \n",
       "15  {'temperature': '0.6', 'top_p': '0.9', 'top_k'...   \n",
       "16  {'temperature': '1.0', 'top_p': '1.0', 'top_k'...   \n",
       "17  {'temperature': '0.0', 'top_p': '0.9', 'max_to...   \n",
       "18  {'temperature': '1.0', 'top_p': '1.0', 'top_k'...   \n",
       "19  {'temperature': '1.0', 'top_p': '1.0', 'top_k'...   \n",
       "20  {'temperature': '1.0', 'top_p': '1.0', 'top_k'...   \n",
       "\n",
       "                                      selected_params  \\\n",
       "0   {'temperature': '1.0', 'top_p': '0.7', 'presen...   \n",
       "1   {'temperature': '1.0', 'top_p': '0.7', 'presen...   \n",
       "2   {'temperature': '1.0', 'top_p': '0.7', 'presen...   \n",
       "3   {'temperature': '1.0', 'max_tokens': '256', 't...   \n",
       "4   {'temperature': '1.0', 'max_tokens': '256', 't...   \n",
       "5   {'temperature': '1.0', 'max_tokens': '256', 't...   \n",
       "6   {'temperature': '1.0', 'top_p': '0.9', 'top_k'...   \n",
       "7   {'temperature': '1.0', 'top_p': '1.0', 'presen...   \n",
       "8   {'temperature': '1.0', 'top_p': '1.0', 'presen...   \n",
       "9   {'temperature': '1.0', 'top_p': '1.0', 'presen...   \n",
       "10  {'temperature': '1.0', 'top_p': '0.9', 'top_k'...   \n",
       "11  {'temperature': '1.0', 'top_p': '0.0', 'max_to...   \n",
       "12  {'temperature': '1.0', 'top_p': '0.0', 'max_to...   \n",
       "13  {'temperature': '1.0', 'top_p': '0.9', 'top_k'...   \n",
       "14  {'temperature': '1.0', 'top_p': '0.9', 'top_k'...   \n",
       "15  {'temperature': '1.0', 'top_p': '0.9', 'top_k'...   \n",
       "16  {'temperature': '1.0', 'top_p': '0.9', 'top_k'...   \n",
       "17  {'temperature': '1.0', 'top_p': '0.9', 'max_to...   \n",
       "18  {'temperature': '1.0', 'top_p': '0.9', 'top_k'...   \n",
       "19  {'temperature': '1.0', 'top_p': '0.9', 'top_k'...   \n",
       "20  {'temperature': '1.0', 'top_p': '0.9', 'top_k'...   \n",
       "\n",
       "                                               header  \\\n",
       "0   You are a conversational assistant. Limit your...   \n",
       "1   You are a conversational assistant. Limit your...   \n",
       "2   You are a conversational assistant. Limit your...   \n",
       "3   You are a conversational assistant. Limit your...   \n",
       "4   You are a conversational assistant. Limit your...   \n",
       "5   You are a conversational assistant. Limit your...   \n",
       "6   You are a conversational assistant. The conver...   \n",
       "7   You are a conversational assistant. Limit your...   \n",
       "8   You are a conversational assistant. Limit your...   \n",
       "9   You are a conversational assistant. Limit your...   \n",
       "10  You are a conversational assistant. The conver...   \n",
       "11  You are a conversational assistant. The conver...   \n",
       "12  You are a conversational assistant. The conver...   \n",
       "13  You are a conversational assistant. Limit your...   \n",
       "14  You are a conversational assistant. Limit your...   \n",
       "15  You are a conversational assistant. Limit your...   \n",
       "16  You are a conversational assistant. The conver...   \n",
       "17  You are a conversational assistant. Limit your...   \n",
       "18  You are a conversational assistant. The conver...   \n",
       "19  You are a conversational assistant. The conver...   \n",
       "20  You are a conversational assistant. The conver...   \n",
       "\n",
       "                                                 link in_yaml params_tested  \\\n",
       "0   https://docs.anthropic.com/claude/docs/legacy-...     Yes           Yes   \n",
       "1   https://docs.anthropic.com/claude/docs/legacy-...     Yes           Yes   \n",
       "2   https://docs.anthropic.com/claude/docs/legacy-...     Yes           Yes   \n",
       "3                 https://docs.cohere.com/docs/models     Yes           Yes   \n",
       "4                 https://docs.cohere.com/docs/models     Yes           Yes   \n",
       "5                 https://docs.cohere.com/docs/models     Yes           Yes   \n",
       "6           https://huggingface.co/google/flan-t5-xxl     Yes           Yes   \n",
       "7    https://platform.openai.com/docs/models/gpt-3-5      Yes           Yes   \n",
       "8   https://platform.openai.com/docs/models/gpt-4-...     Yes           Yes   \n",
       "9   https://platform.openai.com/docs/models/gpt-4-...     Yes           Yes   \n",
       "10  https://huggingface.co/HuggingFaceH4/zephyr-7b...     Yes           Yes   \n",
       "11  https://docs.aleph-alpha.com/docs/introduction...     Yes           Yes   \n",
       "12  https://docs.aleph-alpha.com/docs/introduction...     Yes           Yes   \n",
       "13  https://huggingface.co/meta-llama/Llama-2-13b-...     Yes           Yes   \n",
       "14  https://huggingface.co/meta-llama/Llama-2-70b-...     Yes           Yes   \n",
       "15  https://huggingface.co/meta-llama/Llama-2-7b-c...     Yes           Yes   \n",
       "16  https://huggingface.co/mistralai/Mistral-7B-In...     Yes           Yes   \n",
       "17                    https://ai.google.dev/palm_docs     Yes           Yes   \n",
       "18  https://huggingface.co/OpenAssistant/oasst-sft...     Yes           Yes   \n",
       "19   https://huggingface.co/tiiuae/falcon-7b-instruct     Yes           Yes   \n",
       "20  https://huggingface.co/timdettmers/guanaco-33b...     Yes           Yes   \n",
       "\n",
       "   endpoint_live                                            status  \n",
       "0            Yes                             Deployed Successfully  \n",
       "1            Yes                             Deployed Successfully  \n",
       "2            Yes                             Deployed Successfully  \n",
       "3            Yes                             Deployed Successfully  \n",
       "4            Yes                             Deployed Successfully  \n",
       "5            Yes                             Deployed Successfully  \n",
       "6            Yes                             Deployed Successfully  \n",
       "7            Yes                             Deployed Successfully  \n",
       "8            Yes                             Deployed Successfully  \n",
       "9            Yes                             Deployed Successfully  \n",
       "10           Yes                                          Deployed  \n",
       "11           Yes                             Deployed Successfully  \n",
       "12           Yes                             Deployed Successfully  \n",
       "13           Yes  Deployed Successfully (but fails intermittently)  \n",
       "14           Yes                             Deployed Successfully  \n",
       "15           Yes                             Deployed Successfully  \n",
       "16           Yes                             Deployed Successfully  \n",
       "17           Yes                             Deployed Successfully  \n",
       "18           Yes                             Deployed Successfully  \n",
       "19           Yes                             Deployed Successfully  \n",
       "20           Yes                             Deployed Successfully  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list of records\n",
    "OUTPUT_PATH = f\"{PROJECT_ROOT}/data\"\n",
    "save_as_jsonl(model_df, f\"{OUTPUT_PATH}/models.jsonl\", is_already_records=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a simple mapping too\n",
    "model_mapping = model_df[[\"long_name\", \"short_name\", \"longcode\", \"model_family\"]].copy()\n",
    "# Save as csv\n",
    "model_mapping.to_csv(f\"{OUTPUT_PATH}/storage/mappings/model_mapping.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "griffin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
